{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report: Robot Arm Motion Planning with Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "This project focuses on solving the problem of robotic arm motion planning using Reinforcement Learning (RL). The robotic arm works in the PandaReachDense-v3 environment, which simulates a real-world task of reaching a specified target in a dense reward setting.\n",
    "\n",
    "The problem can be seen as teaching the robotic arm how to move in a controlled and efficient manner to reach a target. This involves using RL to allow the agent to learn from its interactions with the environment.\n",
    "\n",
    "The project is divided into three main parts:\n",
    "\n",
    "* Training the model from scratch.\n",
    "* Fine-tuning the pre-trained model.\n",
    "* Running and evaluating the fine-tuned model in the environment.\n",
    "\n",
    "At first we had a completely different idea for the project. We were to create our own PPO training model with pytorch and use pybullet as an environment. So we got working, and combining torch and RL to pybullet was trickier than we originally thought. Later we scrapped this idea and chose a pretrained model from stable-baselines3 to train and fine-tune on. We also changed the environment to a gym environment, that uses a model from panda_gym. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The training process is the foundation of this project, the agent learns to solve the task by interacting with the environment. The goal of training is to create a policy that enables the robotic arm to reach its target effectively.\n",
    "\n",
    "Dense rewards provide the agent with immediate feedback based on how close it is to the target, allowing it to adjust its actions effectively.\n",
    "\n",
    "For this task, we chose the Proximal Policy Optimization (PPO) algorithm, which is well-suited for continuous problems like this one. PPO is known for its stability and efficiency, making it ideal for tasks requiring precise control.\n",
    "\n",
    "The model was initialized with the following settings:\n",
    "\n",
    "* Policy Type: MultiInputPolicy to handle environments with complex observation spaces.\n",
    "* Verbosity: Set to 1 to display training progress.\n",
    "\n",
    "Training was made over 100,000 timesteps, and during that, the agent interacted with the environment to maximize cumulative rewards. The process involved:\n",
    "\n",
    "* The agent observing the state of the environment.\n",
    "* The agent taking an action based on its current policy.\n",
    "* The environment providing feedback in the form of rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "\n",
    "LOG_DIR = \"./ppo_logs/\"\n",
    "MODEL_PATH = \"ppo_robot_arm.zip\"\n",
    "\n",
    "def train_model(env_id=\"PandaReachDense-v3\", total_timesteps=100000):\n",
    "    \"\"\"\n",
    "    Train a PPO model on the specified environment.\n",
    "\n",
    "    Args:\n",
    "        env_id (str): The environment ID.\n",
    "        total_timesteps (int): Number of timesteps for training.\n",
    "    \"\"\"\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"Creating environment: {env_id}\")\n",
    "    env = gym.make(env_id)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    print(\"Initializing PPO model...\")\n",
    "    model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    print(f\"Saving the model to {MODEL_PATH}...\")\n",
    "    model.save(MODEL_PATH)\n",
    "\n",
    "    print(\"Testing the trained model...\")\n",
    "    obs = env.reset()\n",
    "\n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "The objective of fine-tuning is to leverage the knowledge gained during the initial training phase while refining the model’s behavior in the environment. This approach allows for:\n",
    "\n",
    "* Improved task-specific performance.\n",
    "* Reduced training time compared to training from scratch.\n",
    "* The ability to adapt the model to changes in environment parameters or reward structures\n",
    "\n",
    "The model was initialized with fine-tuning-specific hyperparameters, such as:\n",
    "\n",
    "* Learning rate: \n",
    "1\n",
    "×\n",
    "1\n",
    "0\n",
    "^\n",
    "−\n",
    "4\n",
    "  (slower to avoid overwriting learned knowledge).\n",
    "* Batch size: 64 (for efficient training without overwhelming memory).\n",
    "* Discount factor (gamma): 0.98 (slightly lower to prioritize near-term rewards).\n",
    "\n",
    "To monitor performance improvements, the EvalCallback is used. It saved the best-performing model and logged evaluation metrics at regular intervals.\n",
    "\n",
    "A custom evaluation function is implemented here to measure rewards over five episodes. This function helped validate whether fine-tuning improved the model’s ability to reach the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import os\n",
    "\n",
    "# Path to the saved model\n",
    "SAVED_MODEL_PATH = \"ppo_robot_arm.zip\"\n",
    "FINE_TUNED_MODEL_PATH = \"ppo_robot_arm_fine_tuned.zip\"\n",
    "\n",
    "# Hyperparameters for fine-tuning\n",
    "FINE_TUNE_PARAMS = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 64,\n",
    "    \"gamma\": 0.98,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"ent_coef\": 0.01\n",
    "}\n",
    "\n",
    "# Fine-tuning script\n",
    "def fine_tune(env_id=\"PandaReachDense-v3\", total_timesteps=100000):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained PPO model on a specified environment.\n",
    "\n",
    "    Args:\n",
    "        env_id (str): The environment ID (default is PandaReachDense-v3).\n",
    "        total_timesteps (int): Number of timesteps for fine-tuning.\n",
    "    \"\"\"\n",
    "    # Create the environment and wrap with Monitor\n",
    "    print(f\"Creating environment: {env_id}\")\n",
    "    base_env = gym.make(env_id)\n",
    "    env = make_vec_env(lambda: Monitor(base_env), n_envs=1)\n",
    "\n",
    "    if not os.path.exists(SAVED_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Saved model not found at {SAVED_MODEL_PATH}\")\n",
    "\n",
    "    print(\"Loading pre-trained model...\")\n",
    "    model = PPO.load(SAVED_MODEL_PATH, env=env, **FINE_TUNE_PARAMS)\n",
    "\n",
    "    def custom_eval():\n",
    "        print(\"Running custom evaluation...\")\n",
    "        eval_rewards = []\n",
    "        for episode in range(5):\n",
    "            obs = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "            eval_rewards.append(total_reward)\n",
    "        print(f\"Evaluation rewards: {eval_rewards}\")\n",
    "        print(f\"Average reward: {sum(eval_rewards) / len(eval_rewards)}\")\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=\"./logs/\",\n",
    "                                  log_path=\"./logs/\",\n",
    "                                  eval_freq=10000,\n",
    "                                  deterministic=True,\n",
    "                                  render=False)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "\n",
    "    # Custom evaluation\n",
    "    custom_eval()\n",
    "\n",
    "    # Save the fine-tuned modelö\n",
    "    print(f\"Saving fine-tuned model to {FINE_TUNED_MODEL_PATH}...\")\n",
    "    model.save(FINE_TUNED_MODEL_PATH)\n",
    "    print(\"Fine-tuning completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fine_tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "When the fine-tuned model is saved, it is loaded and tested in the environment. This involves observing how well the agent performes the task of reaching the target while rendering the environment in real time.\n",
    "\n",
    "During this phase, the model predicts actions based on observations, and the robotic arm moves accordingly. The rendering helpes to visualize the agent’s behavior and evaluate its success.\n",
    "\n",
    "The environment was my own main task for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py\n",
    "\n",
    "I chose to include this script here because it contains everything togheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.training_script import train_model\n",
    "from trainer.fine_tuning import fine_tune\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to manage the training, fine-tuning, and running the environment.\n",
    "    \"\"\"\n",
    "    print(\"Starting the Robot Arm Motion Planning Project!\")\n",
    "\n",
    "    # Step 1: Training the model from scratch\n",
    "    print(\"\\nStep 1: Training the model...\")\n",
    "    try:\n",
    "        train_model(env_id=\"PandaReachDense-v3\", total_timesteps=100000)\n",
    "        print(\"Model training completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Fine-tuning the pre-trained model\n",
    "    print(\"\\nStep 2: Fine-tuning the model...\")\n",
    "    try:\n",
    "        fine_tune(env_id=\"PandaReachDense-v3\", total_timesteps=50000)\n",
    "        print(\"Model fine-tuning completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during fine-tuning: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Running the environment using the fine-tuned model\n",
    "    print(\"\\nStep 3: Running the environment with the fine-tuned model...\")\n",
    "    try:\n",
    "        env = gym.make(\"PandaReachDense-v3\", render_mode=\"human\")\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        from stable_baselines3 import PPO\n",
    "        model = PPO.load(\"ppo_robot_arm_fine_tuned.zip\", env=env)\n",
    "\n",
    "        for _ in range(1000):\n",
    "            time.sleep(0.3)\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info, _ = env.step(action)\n",
    "            env.render()\n",
    "            if done:\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "        print(\"Environment run completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during environment run: {e}\")\n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "So how did the fine-tuning and training improve the model?\n",
    "\n",
    "Before the training had been applied i used a different model. The model was acting very randomly then and really didn't have the logic there at all, as can be seen in the enviorment file \"robot_arm_env_gym.py. Then we tested the current model and the same thing applied to this one as well.\n",
    "\n",
    "After the training the model and fine-tuning it, you could immidiatly see a drastic change. This time we also decided to slow down the arm to better see how it did perform. it is not 100% accurate because you can see the flaws when moving the ball/object from time to time, but it is an improvement nonetheless.\n",
    "\n",
    "If we were to improve it further we could try:\n",
    "\n",
    "* Changing the amount of time steps\n",
    "* Experiment with the fine tuning parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "So what did the project give in the end?\n",
    "\n",
    "This project demonstrated the effectiveness of this PPO algorithm for robotic arm motion planning. By combining training from scratch and fine-tuning, the model achieved reliable performance in solving the task in the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
